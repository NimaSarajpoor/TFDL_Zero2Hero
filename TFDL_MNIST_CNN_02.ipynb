{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64b0563",
   "metadata": {},
   "source": [
    "For computer vision, there is a popular kind of Neural Network, known as Convolutional Neural Network (CNN). Two important concepts of CNN:\n",
    "* Convolution --> Creating a sliding window (m-by-n), called Kernel, that moves through an image (M-by-N) and results in a new \"image\".\n",
    "\n",
    "* Max pooling --> We again need a window, but now we are not convolving. We basically get \"max\" of values enclosd by that window. This process results in a new \"image\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf325fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93743caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n",
       " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d58f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[0]\n",
    "data_test = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9605ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[0] / 255\n",
    "y_train = data_train[1]\n",
    "\n",
    "X_test = data_test[0] / 255\n",
    "y_test = data_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e9422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65090d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "])\n",
    "\n",
    "# Exercise:\n",
    "# Try to understand how these layers are connected. like..what is 32? \n",
    "# or, how does MaxPool affect the shape of data...? etc.\n",
    "# what are the parameters? ALl will be revealed in \"math\" parts...\n",
    "# (feel free to search online. But, if you are serious, \n",
    "# go and checkout and math notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e36b7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics='accuracy',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5368e8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               1605760   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,866\n",
      "Trainable params: 1,625,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce76ef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.1114 - accuracy: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1e77a2fd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80e70b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = model.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "209b410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68ac2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% plus of your work is just about cleaning data! \n",
    "# So, it is very important, particularly in real world problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a419d0",
   "metadata": {},
   "source": [
    "Let us plot mislabeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfb6a1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  63,   78,  115,  211,  259,  266,  340,  445,  448,  495,  543,\n",
       "        582,  591,  659,  674,  691,  717,  740,  813,  829,  839,  844,\n",
       "        882,  924,  965, 1014, 1039, 1178, 1182, 1202, 1226, 1232, 1234,\n",
       "       1242, 1247, 1260, 1289, 1299, 1319, 1326, 1364, 1393, 1502, 1530,\n",
       "       1549, 1553, 1562, 1621, 1686, 1709, 1717, 1737, 1754, 1774, 1790,\n",
       "       1828, 1856, 1878, 1901, 1955, 2009, 2035, 2043, 2053, 2093, 2107,\n",
       "       2109, 2118, 2129, 2130, 2135, 2182, 2266, 2272, 2369, 2382, 2387,\n",
       "       2395, 2414, 2447, 2526, 2654, 2742, 2758, 2810, 2896, 2921, 2939,\n",
       "       2953, 2995, 3023, 3030, 3073, 3130, 3206, 3422, 3503, 3520, 3534,\n",
       "       3558, 3559, 3662, 3751, 3757, 3767, 3780, 3808, 3853, 3902, 3941,\n",
       "       3968, 3976, 4063, 4075, 4078, 4140, 4163, 4176, 4207, 4224, 4256,\n",
       "       4294, 4350, 4360, 4380, 4443, 4497, 4507, 4536, 4571, 4575, 4601,\n",
       "       4639, 4679, 4731, 4761, 4783, 4807, 4860, 4890, 4956, 4978, 5068,\n",
       "       5246, 5547, 5676, 5749, 5752, 5888, 5937, 5955, 5982, 5997, 6028,\n",
       "       6071, 6081, 6091, 6166, 6168, 6505, 6555, 6560, 6571, 6578, 6597,\n",
       "       6625, 6641, 6651, 6755, 6783, 7220, 7259, 7265, 7432, 7434, 7524,\n",
       "       7545, 7603, 7735, 7800, 7813, 7821, 7853, 7856, 7862, 7899, 7921,\n",
       "       7923, 7928, 7991, 8059, 8065, 8095, 8105, 8183, 8316, 8332, 8408,\n",
       "       8519, 8522, 9009, 9015, 9019, 9024, 9028, 9051, 9057, 9280, 9540,\n",
       "       9587, 9612, 9634, 9638, 9642, 9664, 9679, 9692, 9729, 9770, 9782,\n",
       "       9792, 9888, 9892], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = y_test != y_test_pred\n",
    "misclassified_idx = np.flatnonzero(mask)\n",
    "misclassified_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54b15178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example\n",
    "idx = 78\n",
    "plt.imshow(X_test[idx])\n",
    "plt.title(f\"pred: {y_test_pred[idx]} (true label: {y_test[idx]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cb998",
   "metadata": {},
   "source": [
    "**What if** the images have different sizes? How should we deal with that?\n",
    "<br>\n",
    "**Solution:** Resize all images into the same size :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7267ae",
   "metadata": {},
   "source": [
    "So, we can do: <br>\n",
    "\n",
    "`tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu, input_shape=(150, 150, 3))`\n",
    "\n",
    "<br>\n",
    "\n",
    "Then, we can see that we are resizing all images to (150, 150). But, what is that `3` in (150, 150, 3)? That means we are using 3 channels (for RGB). So, the image is not gray scale. Note that in image with 3 channels RGB, our kernel (filter) will be 3D, and the filter in each channel can be different that filter of another channel. However, we can then add the impact of 3 filters (of 3D kernel) on 3 channel and return a single value. So, the result can be 2D image. Now, if I have `n` 3D-kernels...I will get `n` 2D images, and thus my new object has L\\*W\\*D where L*W is the same as size of 2D image, and D is `n`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268ba50",
   "metadata": {},
   "source": [
    "**Let us take a look again at the code:** <br>\n",
    "\n",
    "`tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu, input_shape=(28, 28, 1))`\n",
    "\n",
    "This line shows that we have 32 filters and our kernel is 3-by-3. We then use input image of 28\\*28\\*1, which means our image is 2D (with one channel). Note that the stride, by default (see doc), is (1,1). And `padding` is set `same`. Therefore, according to doc, the output will have the same image size, so 28\\*28. However, have 32 filters. So, it would be (28, 28, 32). How about number of neurons? How does NN look like? It will be taken care of shortly in upcoming notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d5738",
   "metadata": {},
   "source": [
    "`tf.keras.layers.MaxPool2D((2, 2), strides=2)` <br>\n",
    "Note that here we do not have number of filters. Because, in maxpooling, we are change number of channels (depth), but we just sliding window and change the size of each 2D image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb58eea",
   "metadata": {},
   "source": [
    "Perform TFDL on Cat-Dog image classification later:\n",
    "[link](https://learn.udacity.com/courses/ud187/lessons/4a041ac9-3bb9-43d6-8a02-4fa912626028/concepts/610653f2-f9a7-43fa-a8c8-d166eea9c4ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a8693",
   "metadata": {},
   "source": [
    "**Note:** For binary classification, our last layer can be:\n",
    "* 2 nodes, with `softmax` as activation function\n",
    "* 1 node, with `sigmoid` as activation function, loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab1a66",
   "metadata": {},
   "source": [
    "**Overfitting:**\n",
    "In overfitting, the model is not fitted too well (too much) on a train data as if it memorizes them rather than learning the general behavior. Therefore, we need a way to resolve this overfitting challenge. One way is to apply it on validation set and then choose the model that results in low error in validation test. We may perform cross-validation. \n",
    "\n",
    "Note that, even in this case, we may end up being biased towards our validation test. So, we need a test set that we do not touch, and then finally just evaluate the performance of chosen model on new, completely unseen data.\n",
    "\n",
    "Two important approaches for dealing with overfitting challenge are:\n",
    "* Data Augmentation -> creating differenet versions of image (like flipping, rotation, etc) to help model genralize better\n",
    "* Drop out -> randomly remove some of neurons to help others get chance to train better and get generalized better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ace21",
   "metadata": {},
   "source": [
    "**Exercise:** <br>\n",
    "After performing simple TFDL on  cat-dog data, you can try to consider `Data Augmentation` (in preprocessing) and/or `Drop out`(throughout learning process) to see the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf8476",
   "metadata": {},
   "source": [
    "Another way to avoid overfitting is early stopping... It means we stop the learning process at early stages to let model not be overfitted to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eecc75",
   "metadata": {},
   "source": [
    "**Exercise:** <br>\n",
    "Use CNN to classify any other dataset of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5db6ad",
   "metadata": {},
   "source": [
    "**Suggestion:** Before the exercises, you may want to take a look at the next few notebooks where I talk about details of NN, and explain more about the model and the tf inputs. We can then come back here to do exercises and move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72bcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
